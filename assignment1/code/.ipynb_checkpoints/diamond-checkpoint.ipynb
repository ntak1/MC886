{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC886 Assigment 1\n",
    "\n",
    "## Objective:\n",
    "Use linear regression model to predict the prices of diamonds given their atributes. The data set might be found on:\n",
    "https://www.kaggle.com/shivam2503/diamonds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/diamonds.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir, 'rb') as csvfile:\n",
    "    dataset = pd.read_csv(data_dir)\n",
    "    \n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carat\tcut\tcolor\tclarity\tdepth\ttable\tprice\tx\ty\tz\n",
    "# print(dataset['cut'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "### 1. Split data in training, validation and test\n",
    "A wise person said: \"friends donâ€™t let friends use testing data for training\". t.\n",
    "\n",
    "**Note:**\n",
    "* **sklearn.model_selection.train_test_split(*arrays, **options)**  \n",
    "    Split arrays or matrices into random train and test subsets.  \n",
    "    See documentation:http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "### 2. Put the dataset into Numpy volume\n",
    "Categorical atributes are changed to integers.  \n",
    "The cell bellow runs a code to shuffle and split the dataset into 3 sets, training, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS ONLY ONCE!\n",
    "# this code saves the numpy vectors to files\n",
    "# split_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 30000)\n",
      "(1, 30000)\n",
      "(9, 10000)\n",
      "(1, 10000)\n",
      "(9, 13940)\n",
      "(1, 13940)\n"
     ]
    }
   ],
   "source": [
    "#load the numpy files (training set, validation set, test set)\n",
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_vectors()\n",
    "y_train = y_train.reshape((1,y_train.shape[0]))\n",
    "y_validation = y_validation.reshape((1,y_validation.shape[0]))\n",
    "y_test = y_test.reshape((1,y_test.shape[0]))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(X):\n",
    "    \"\"\"\n",
    "    Receives: the input vector shape (nx, m), where nx is the number of features and\n",
    "    m the number of examples.\n",
    "    Returns:\n",
    "    W: the weight array shape ( 1, nx)\n",
    "    dW: the derivative array shape ( 1, nx)\n",
    "    b: a real number (bias)\n",
    "    db: the derivative of the bias.\n",
    "    \"\"\"\n",
    "    nx = X.shape[0]\n",
    "    W = np.random.randn(nx,1)*0.01\n",
    "    dw = np.random.randn(nx,1)*0.01\n",
    "    b = 0.\n",
    "    db = 0.\n",
    "    return W,b, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_cost(X, Y, Y_hat, m):\n",
    "    \"\"\"\n",
    "    Cost function defined by a variation of the l2 norm function.\n",
    "    The '2' in the denominator makes its deriative easier\n",
    "    \"\"\"\n",
    "    cost = (1/(2*m))*np.sum((Y-Y_hat)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORWARD PROPAGATION (WELL THIS IS NOT A NN, BUT ...)\n",
    "def forward_prop(X, w, b):\n",
    "    \"\"\"\n",
    "    X is the input vector with m training examples, shape (1,m)\n",
    "    w: is the angular coefficient shape (9,1)\n",
    "    b: is the linear coefficient  shape (9,1)\n",
    "    \"\"\"\n",
    "    Y_hat = np.dot(w.T, X) + b\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X,Y, Y_hat, m):\n",
    "    \"\"\"\n",
    "    Calculates the derivatives of the parameters\n",
    "    returns:\n",
    "    dw\n",
    "    db\n",
    "    \"\"\"\n",
    "    nx = X.shape[0]\n",
    "    ny = Y.shape[0]\n",
    "    dw = (-1./m)*np.dot(X,(Y -Y_hat).T)\n",
    "    db = (-1./m)*np.sum((Y -Y_hat))\n",
    "    assert(dw.shape == (nx, ny))\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_model(X,Y, num_iterations = 100, learning_rate = 0.000000001):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input array shape (nx, m), nx is the number of input features and m the number of\n",
    "    training examples.\n",
    "    Y: label array shape (ny, m) where ny is the number of the output feature.\n",
    "    Given X and Y estimates the best linear function to fit the data\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    print('m = ', m) \n",
    "    \n",
    "    w,b,dw,db = init_parameters(x_train)\n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        #forward propagation (we can think the activation function as linear)\n",
    "        Y_hat = forward_prop(X,w,b)\n",
    "        \n",
    "        #plot the cost function\n",
    "        cost = l2_cost(X,Y, Y_hat,m)\n",
    "        plt.plot(iteration,cost,'ro')\n",
    "    \n",
    "        #Backward propagation calculate the derivatives \n",
    "        dw, db = backward_propagation(X,Y,Y_hat, m)\n",
    "    \n",
    "        #Update parameters simultaneusly\n",
    "        temp_w = w - learning_rate*dw\n",
    "        temp_b = b - learning_rate*db\n",
    "        w = temp_w\n",
    "        b = temp_b\n",
    "    plt.show()\n",
    "    print(\"dw.shape = \", dw.shape)\n",
    "    print(\"db.shape = \", db.shape)\n",
    "    print(\"w.shape = \", w.shape)\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m =  30000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw.shape =  (9, 1)\n",
      "db.shape =  ()\n",
      "w.shape =  (9, 1)\n",
      "cost in the training set:  11.470559389888889\n",
      "weights:  [[-0.00321154]\n",
      " [ 0.00196906]\n",
      " [ 0.01025924]\n",
      " [-0.01209524]\n",
      " [ 0.00348157]\n",
      " [ 0.00479994]\n",
      " [-0.00030939]\n",
      " [-0.00453906]\n",
      " [ 0.00556399]]\n",
      "bias:  2.34471874055764e-06\n"
     ]
    }
   ],
   "source": [
    "w_gd, b_gd, cost_gd= GD_model(x_train,y_train)\n",
    "print(\"cost in the training set: \", cost_gd)\n",
    "print(\"weights: \", w_gd)\n",
    "print(\"bias: \", b_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_model(X,Y, num_iterations = 20, learning_rate = 0.00000001):\n",
    "    \"\"\"\n",
    "    X: an input array shape (nx, m), nx is the input feature size and m is the number of training examples\n",
    "    Given X and Y estimates the best linear function to fit the data\n",
    "    \"\"\"\n",
    "    # initialize parameters\n",
    "    w,b,dw,db = init_parameters(x_train)\n",
    "    print(\"first \", dw)\n",
    "    m = X.shape[1]    # number of training examples\n",
    "    nx = X.shape[0]   # number of features in the input\n",
    "    ny = Y.shape[0]   # number of features in the label\n",
    "    \n",
    "    # for each epoch\n",
    "    for iteration in range(num_iterations):\n",
    "        print(\"\\riteraÃ§Ã£o: \", iteration +1, end = \"\")\n",
    "        #for each element in the example's vector (shape (9,1))\n",
    "        Y_hat = np.empty((ny, m))\n",
    "        for i in range(m):\n",
    "            #extract a columns of the input, remember the ith column has the ith example\n",
    "            x_i = X[:, i].reshape(nx,1) #reshape to avoid rank 1 array\n",
    "            y_i = Y[:, i].reshape(ny,1)\n",
    "#             print(\"x_i.shape = \", x_i.shape)\n",
    "#             print(\"y_i.shape = \", y_i.shape)\n",
    "#             print(\"w.shape = \", w.shape)\n",
    "            \n",
    "            #calulate the prediction y_hat\n",
    "            y_hat_i = np.dot(w.T,x_i) + b\n",
    "            Y_hat[:,i] = y_hat_i\n",
    "#             print(\"y_hat_i.shape = \", y_hat_i.shape)\n",
    "            \n",
    "#             #calculate the derivatives\n",
    "#              dw = (-1./m)*np.dot(X,(Y -Y_hat).T)\n",
    "#     db = (-1./m)*np.sum((Y -Y_hat))\n",
    "            \n",
    "#             dw = \n",
    "            dw, db = backward_propagation(x_i,y_i,y_hat_i, 1)\n",
    "#             print(\"dw.shape = \", dw.shape)\n",
    "#             print(\"db.shape = \", db.shape)\n",
    "            #Update parameters simultaneusly\n",
    "            temp_w = w - learning_rate*dw\n",
    "            temp_b = b - learning_rate*db\n",
    "            w = temp_w\n",
    "            b = temp_b\n",
    "            #plot the cost function\n",
    "#             cost = (1./(2.))*np.sum((label -Y_hat)**2)\n",
    "        cost = l2_cost(X,Y, Y_hat,m)\n",
    "        print(\"cost = \", cost)\n",
    "        plt.plot(iteration,cost,'ro')\n",
    "    plt.show()\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first  [[ 0.0053618 ]\n",
      " [-0.0112733 ]\n",
      " [ 0.00276703]\n",
      " [-0.00115786]\n",
      " [ 0.01826682]\n",
      " [-0.01529661]\n",
      " [ 0.00292724]\n",
      " [-0.01315733]\n",
      " [-0.0067481 ]]\n",
      "iteraÃ§Ã£o:  1cost =  0.6471668129952484\n",
      "iteraÃ§Ã£o:  2cost =  0.20083739877208337\n",
      "iteraÃ§Ã£o:  3cost =  0.13474418780708292\n",
      "iteraÃ§Ã£o:  4cost =  0.12080934049424351\n",
      "iteraÃ§Ã£o:  5cost =  0.11701510686106838\n",
      "iteraÃ§Ã£o:  6cost =  0.11579308689684052\n",
      "iteraÃ§Ã£o:  7cost =  0.11534228766139758\n",
      "iteraÃ§Ã£o:  8cost =  0.11514075691893204\n",
      "iteraÃ§Ã£o:  9cost =  0.11502214617842062\n",
      "iteraÃ§Ã£o:  10cost =  0.114931439981391\n",
      "iteraÃ§Ã£o:  11cost =  0.11485020224490039\n",
      "iteraÃ§Ã£o:  12cost =  0.11477222681455594\n",
      "iteraÃ§Ã£o:  13cost =  0.11469542121671608\n",
      "iteraÃ§Ã£o:  14cost =  0.1146190795055514\n",
      "iteraÃ§Ã£o:  15cost =  0.11454296339327114\n",
      "iteraÃ§Ã£o:  16cost =  0.11446699236433015\n",
      "iteraÃ§Ã£o:  17cost =  0.11439113913462877\n",
      "iteraÃ§Ã£o:  18cost =  0.11431539438068167\n",
      "iteraÃ§Ã£o:  19cost =  0.11423975483921761\n",
      "iteraÃ§Ã£o:  20cost =  0.1141642192919367\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost in the training set:  0.1141642192919367\n",
      "weights:  [[ 0.00944356]\n",
      " [-0.00409872]\n",
      " [-0.01316558]\n",
      " [ 0.00379637]\n",
      " [ 0.02472989]\n",
      " [ 0.02417647]\n",
      " [ 0.00014122]\n",
      " [ 0.00102681]\n",
      " [ 0.00947979]]\n",
      "bias:  0.000207616954192884\n"
     ]
    }
   ],
   "source": [
    "w_sgd, b_sgd, cost_sgd = SGD_model(x_train,y_train)\n",
    "print(\"cost in the training set: \", cost_sgd)\n",
    "print(\"weights: \", w_sgd)\n",
    "print(\"bias: \", b_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [0.00022725 0.00098541 0.00114936 0.00131835 0.02531951 0.02330354\n",
      " 0.00011554 0.0021788  0.00218667]\n",
      "Mean squared error: 0.14\n",
      "Error: 0.07\n",
      "Variance score: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntkm/env/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/home/ntkm/env/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "clf = linear_model.SGDRegressor(loss='squared_loss', penalty='l2', learning_rate = 'constant', eta0 = 0.00000001)\n",
    "clf.fit(x_train.T, y_train.T)\n",
    "# Make predictions using the testing set\n",
    "y_hat = clf.predict(x_train.T)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', clf.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train.T, y_hat.T))\n",
    "train_size = 30000\n",
    "print(\"Error: %.2f\"\n",
    "      % l2_cost(x_train, y_train, y_hat, train_size))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_train.T, y_hat.T))\n",
    "\n",
    "# cost [element] = l2_cost(x_train, y_train, y_)\n",
    "# plt.show()\n",
    "# print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_norm = np.concatenate((x_train, x_validation), axis=1).T\n",
    "\n",
    "x0_norm = np.ones((1, train_size))\n",
    "X_norm = np.concatenate((x0_norm, x_train), axis=0)\n",
    "print(X_norm.shape)\n",
    "# y_norm = np.concatenate((y_train, y_validation), axis=1).T\n",
    "# n_features = X_norm.shape[1]\n",
    "# print('X:', X_norm.shape)\n",
    "# # compute Normal Eq.\n",
    "XTX_inv = np.linalg.inv(np.matmul(X_norm.T, X_norm))\n",
    "theta_norm = np.matmul(np.matmul(XTX_inv, X_norm.T), y_train)\n",
    "# print('theta: ', theta_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X_test_norm = np.concatenate((np.ones((x_test.shape[1], 1)), x_test.T), axis=1)\n",
    "y_test_norm = y_test\n",
    "\n",
    "print('test')\n",
    "print('X test:', X_test_norm.shape)\n",
    "print('y test:', y_test_norm.shape)\n",
    "print('*' * 40)\n",
    "\n",
    "max_diff = 0\n",
    "for i in range(len(X_test_norm)):\n",
    "    estimate = np.matmul(theta_norm.T, np.reshape(X_test_norm[i], (n_features, 1)))\n",
    "    print('i: %d; estimado: %lf; real: %lf; diferenca: %lf' \n",
    "          %(i, estimate, y_test_norm[:,i], estimate-y_test_norm[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypotesis(theta, X):\n",
    "    return np.matmul(theta.T, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epoch = 100\n",
    "batches_per_epoch = int(np.ceil(train_size / float(batch_size)))\n",
    "n_steps = int(round(batches_per_epoch * n_epoch))\n",
    "learning_rate = 0.000000001\n",
    "\n",
    "print('training size: ', train_size)\n",
    "print('number of epoch: ', n_epoch)\n",
    "print('batches per epoch: ', batches_per_epoch)\n",
    "print('number of steps: ', n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "theta_mbgd = np.random.rand(nx + 1, ny)\n",
    "x0_mbgd = np.ones((train_size, 1))\n",
    "X_mbgd = np.concatenate((x0_mbgd, x_train.T), axis=1)\n",
    "print(theta_mbgd.shape,  X_mbgd.shape)\n",
    "\n",
    "for j in range(10):\n",
    "#     np.random.shuffle(X_mbgd)\n",
    "    for i in range(n_epoch):\n",
    "        h = hypotesis(theta_mbgd, X_mbgd[i:i+batch_size, :].T)\n",
    "    #    print(h.shape)\n",
    "        cost = np.sum((h - np.sum(y_train[:, i:i+batch_size]))) / batch_size\n",
    "\n",
    "        # update\n",
    "        theta_mbgd = theta_mbgd - (learning_rate  * cost)\n",
    "#         if j%10000==0 and i == 0:\n",
    "        print('j: %d;\\tcost: %.3lf' %(j, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X_test_mbgd = np.concatenate((np.ones((x_test.shape[1], 1)), x_test.T), axis=1)\n",
    "y_test_mbgd = y_test\n",
    "\n",
    "print('test')\n",
    "print('X test:', X_test_mbgd.shape)\n",
    "print('y test:', y_test_mbgd.shape)\n",
    "print('*' * 40)\n",
    "\n",
    "max_diff = 0\n",
    "for i in range(len(X_test_mbgd)):\n",
    "    estimate = np.matmul(theta_mbgd.T, np.reshape(X_test_mbgd[i], (nx+1, 1)))\n",
    "    print('i: %d; estimado: %.3lf; real: %.3lf; diferenca: %.3lf' \n",
    "          %(i, estimate, y_test_mbgd[:,i], estimate-y_test_mbgd[:,i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
