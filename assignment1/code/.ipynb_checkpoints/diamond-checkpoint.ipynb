{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC886 Assigment 1\n",
    "\n",
    "## Objective:\n",
    "Use linear regression model to predict the prices of diamonds given their atributes. The data set might be found on:\n",
    "https://www.kaggle.com/shivam2503/diamonds.\n",
    "\n",
    "\n",
    "## Activities\n",
    "1.  Perform Linear Regression.  You should implement your solution and compare it with sklearn.linear model.SGDRegressor(“linear  model  fitted  by  minimizing  a  regularized  empirical  loss  with  SGD”).What are the conclusions?  \n",
    "2.  Use the specified training/test data for providing your results and avoid overfitting.Keep in mind that friends don’t let friends use testing data for training.  \n",
    "3.  Plot  the  cost  function  vs.  number  of  iterations  in  the  training  set  and  analyze  the  model  complexity. What are the conclusions?  What are the actions after such analyses?\n",
    "4.  Use different Gradient Descent (GD) learning rates when optimizing.  Compare the GD-based solutionswith Normal Equation.  You should implement your solutions.  What are the conclusions?  \n",
    "5.  Prepare  a  4-page  (max.)  report  with  all  your  findings.  It  is  UP  TO  YOU  to  convince  the  reader  that you are proficient on linear regression and the choices it entails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/diamonds.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir, 'rb') as csvfile:\n",
    "    dataset = pd.read_csv(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0  carat        cut color clarity  depth  table  price     x  \\\n",
      "0               1   0.23      Ideal     E     SI2   61.5   55.0    326  3.95   \n",
      "1               2   0.21    Premium     E     SI1   59.8   61.0    326  3.89   \n",
      "2               3   0.23       Good     E     VS1   56.9   65.0    327  4.05   \n",
      "3               4   0.29    Premium     I     VS2   62.4   58.0    334  4.20   \n",
      "4               5   0.31       Good     J     SI2   63.3   58.0    335  4.34   \n",
      "5               6   0.24  Very Good     J    VVS2   62.8   57.0    336  3.94   \n",
      "6               7   0.24  Very Good     I    VVS1   62.3   57.0    336  3.95   \n",
      "7               8   0.26  Very Good     H     SI1   61.9   55.0    337  4.07   \n",
      "8               9   0.22       Fair     E     VS2   65.1   61.0    337  3.87   \n",
      "9              10   0.23  Very Good     H     VS1   59.4   61.0    338  4.00   \n",
      "10             11   0.30       Good     J     SI1   64.0   55.0    339  4.25   \n",
      "11             12   0.23      Ideal     J     VS1   62.8   56.0    340  3.93   \n",
      "12             13   0.22    Premium     F     SI1   60.4   61.0    342  3.88   \n",
      "13             14   0.31      Ideal     J     SI2   62.2   54.0    344  4.35   \n",
      "14             15   0.20    Premium     E     SI2   60.2   62.0    345  3.79   \n",
      "15             16   0.32    Premium     E      I1   60.9   58.0    345  4.38   \n",
      "16             17   0.30      Ideal     I     SI2   62.0   54.0    348  4.31   \n",
      "17             18   0.30       Good     J     SI1   63.4   54.0    351  4.23   \n",
      "18             19   0.30       Good     J     SI1   63.8   56.0    351  4.23   \n",
      "19             20   0.30  Very Good     J     SI1   62.7   59.0    351  4.21   \n",
      "20             21   0.30       Good     I     SI2   63.3   56.0    351  4.26   \n",
      "21             22   0.23  Very Good     E     VS2   63.8   55.0    352  3.85   \n",
      "22             23   0.23  Very Good     H     VS1   61.0   57.0    353  3.94   \n",
      "23             24   0.31  Very Good     J     SI1   59.4   62.0    353  4.39   \n",
      "24             25   0.31  Very Good     J     SI1   58.1   62.0    353  4.44   \n",
      "25             26   0.23  Very Good     G    VVS2   60.4   58.0    354  3.97   \n",
      "26             27   0.24    Premium     I     VS1   62.5   57.0    355  3.97   \n",
      "27             28   0.30  Very Good     J     VS2   62.2   57.0    357  4.28   \n",
      "28             29   0.23  Very Good     D     VS2   60.5   61.0    357  3.96   \n",
      "29             30   0.23  Very Good     F     VS1   60.9   57.0    357  3.96   \n",
      "...           ...    ...        ...   ...     ...    ...    ...    ...   ...   \n",
      "53910       53911   0.70    Premium     E     SI1   60.5   58.0   2753  5.74   \n",
      "53911       53912   0.57    Premium     E      IF   59.8   60.0   2753  5.43   \n",
      "53912       53913   0.61    Premium     F    VVS1   61.8   59.0   2753  5.48   \n",
      "53913       53914   0.80       Good     G     VS2   64.2   58.0   2753  5.84   \n",
      "53914       53915   0.84       Good     I     VS1   63.7   59.0   2753  5.94   \n",
      "53915       53916   0.77      Ideal     E     SI2   62.1   56.0   2753  5.84   \n",
      "53916       53917   0.74       Good     D     SI1   63.1   59.0   2753  5.71   \n",
      "53917       53918   0.90  Very Good     J     SI1   63.2   60.0   2753  6.12   \n",
      "53918       53919   0.76    Premium     I     VS1   59.3   62.0   2753  5.93   \n",
      "53919       53920   0.76      Ideal     I    VVS1   62.2   55.0   2753  5.89   \n",
      "53920       53921   0.70  Very Good     E     VS2   62.4   60.0   2755  5.57   \n",
      "53921       53922   0.70  Very Good     E     VS2   62.8   60.0   2755  5.59   \n",
      "53922       53923   0.70  Very Good     D     VS1   63.1   59.0   2755  5.67   \n",
      "53923       53924   0.73      Ideal     I     VS2   61.3   56.0   2756  5.80   \n",
      "53924       53925   0.73      Ideal     I     VS2   61.6   55.0   2756  5.82   \n",
      "53925       53926   0.79      Ideal     I     SI1   61.6   56.0   2756  5.95   \n",
      "53926       53927   0.71      Ideal     E     SI1   61.9   56.0   2756  5.71   \n",
      "53927       53928   0.79       Good     F     SI1   58.1   59.0   2756  6.06   \n",
      "53928       53929   0.79    Premium     E     SI2   61.4   58.0   2756  6.03   \n",
      "53929       53930   0.71      Ideal     G     VS1   61.4   56.0   2756  5.76   \n",
      "53930       53931   0.71    Premium     E     SI1   60.5   55.0   2756  5.79   \n",
      "53931       53932   0.71    Premium     F     SI1   59.8   62.0   2756  5.74   \n",
      "53932       53933   0.70  Very Good     E     VS2   60.5   59.0   2757  5.71   \n",
      "53933       53934   0.70  Very Good     E     VS2   61.2   59.0   2757  5.69   \n",
      "53934       53935   0.72    Premium     D     SI1   62.7   59.0   2757  5.69   \n",
      "53935       53936   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75   \n",
      "53936       53937   0.72       Good     D     SI1   63.1   55.0   2757  5.69   \n",
      "53937       53938   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66   \n",
      "53938       53939   0.86    Premium     H     SI2   61.0   58.0   2757  6.15   \n",
      "53939       53940   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83   \n",
      "\n",
      "          y     z  \n",
      "0      3.98  2.43  \n",
      "1      3.84  2.31  \n",
      "2      4.07  2.31  \n",
      "3      4.23  2.63  \n",
      "4      4.35  2.75  \n",
      "5      3.96  2.48  \n",
      "6      3.98  2.47  \n",
      "7      4.11  2.53  \n",
      "8      3.78  2.49  \n",
      "9      4.05  2.39  \n",
      "10     4.28  2.73  \n",
      "11     3.90  2.46  \n",
      "12     3.84  2.33  \n",
      "13     4.37  2.71  \n",
      "14     3.75  2.27  \n",
      "15     4.42  2.68  \n",
      "16     4.34  2.68  \n",
      "17     4.29  2.70  \n",
      "18     4.26  2.71  \n",
      "19     4.27  2.66  \n",
      "20     4.30  2.71  \n",
      "21     3.92  2.48  \n",
      "22     3.96  2.41  \n",
      "23     4.43  2.62  \n",
      "24     4.47  2.59  \n",
      "25     4.01  2.41  \n",
      "26     3.94  2.47  \n",
      "27     4.30  2.67  \n",
      "28     3.97  2.40  \n",
      "29     3.99  2.42  \n",
      "...     ...   ...  \n",
      "53910  5.77  3.48  \n",
      "53911  5.38  3.23  \n",
      "53912  5.40  3.36  \n",
      "53913  5.81  3.74  \n",
      "53914  5.90  3.77  \n",
      "53915  5.86  3.63  \n",
      "53916  5.74  3.61  \n",
      "53917  6.09  3.86  \n",
      "53918  5.85  3.49  \n",
      "53919  5.87  3.66  \n",
      "53920  5.61  3.49  \n",
      "53921  5.65  3.53  \n",
      "53922  5.58  3.55  \n",
      "53923  5.84  3.57  \n",
      "53924  5.84  3.59  \n",
      "53925  5.97  3.67  \n",
      "53926  5.73  3.54  \n",
      "53927  6.13  3.54  \n",
      "53928  5.96  3.68  \n",
      "53929  5.73  3.53  \n",
      "53930  5.74  3.49  \n",
      "53931  5.73  3.43  \n",
      "53932  5.76  3.47  \n",
      "53933  5.72  3.49  \n",
      "53934  5.73  3.58  \n",
      "53935  5.76  3.50  \n",
      "53936  5.75  3.61  \n",
      "53937  5.68  3.56  \n",
      "53938  6.12  3.74  \n",
      "53939  5.87  3.64  \n",
      "\n",
      "[53940 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# carat\tcut\tcolor\tclarity\tdepth\ttable\tprice\tx\ty\tz\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "### 1. Split data in training, validation and test\n",
    "A wise person said: \"friends don’t let friends use testing data for training\". t.\n",
    "\n",
    "**Note:**\n",
    "* **sklearn.model_selection.train_test_split(*arrays, **options)**  \n",
    "    Split arrays or matrices into random train and test subsets.  \n",
    "    See documentation:http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "### 2. Put the dataset into Numpy volume\n",
    "Categorical atributes are changed to integers.  \n",
    "The cell bellow runs a code to shuffle and split the dataset into 3 sets, training, validation and test.\n",
    "\n",
    "### 3. Transform categorical data into numbers\n",
    "Used fit_transform function.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html  \n",
    "http://pbpython.com/categorical-encoding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS ONLY ONCE!\n",
    "#this code saves the numpy vectors to files, if the files already exists, don't run\n",
    "#split_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  (9, 30000)\n",
      "y_train.shape =  (1, 30000)\n",
      "x_validation.shape =  (9, 10000)\n",
      "y_validation.shape =  (1, 10000)\n",
      "x_test.shape =  (9, 13940)\n",
      "y_test.shape =  (1, 13940)\n",
      "prices =  [[1760.  506. 2898. ... 1577. 1087.  658.]]\n",
      "an input example =  [ 0.54  2.    3.    4.   62.2  56.    5.23  5.19  3.24]\n"
     ]
    }
   ],
   "source": [
    "#load the numpy files (training set, validation set, test set)\n",
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_vectors()\n",
    "y_train = y_train.reshape((1,y_train.shape[0]))\n",
    "y_validation = y_validation.reshape((1,y_validation.shape[0]))\n",
    "y_test = y_test.reshape((1,y_test.shape[0]))\n",
    "\n",
    "print(\"x_train.shape = \", x_train.shape)\n",
    "print(\"y_train.shape = \", y_train.shape)\n",
    "print(\"x_validation.shape = \", x_validation.shape)\n",
    "print(\"y_validation.shape = \", y_validation.shape)\n",
    "print(\"x_test.shape = \", x_test.shape)\n",
    "print(\"y_test.shape = \", y_test.shape)\n",
    "print(\"prices = \", y_train)\n",
    "print(\"an input example = \", x_train[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(X):\n",
    "    \"\"\"\n",
    "    Receives: the input vector shape (nx, m), where nx is the number of features and\n",
    "    m the number of examples.\n",
    "    Returns:\n",
    "    W: the weight array shape ( 1, nx)\n",
    "    dW: the derivative array shape ( 1, nx)\n",
    "    b: a vector shape (nx,1)\n",
    "    db: the derivative of the bias.\n",
    "    \"\"\"\n",
    "    nx = X.shape[0]\n",
    "    W = np.random.randn(nx,1)*0.01\n",
    "    dw = np.random.randn(nx,1)*0.01\n",
    "    b = 0.\n",
    "    db = 0.\n",
    "    return W,b, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_cost(X, Y, Y_hat, m):\n",
    "    \"\"\"\n",
    "    Cost function defined by a variation of the l2 norm function.\n",
    "    The '2' in the denominator makes its deriative easier\n",
    "    \"\"\"\n",
    "    cost = (1.0/(2*m))*np.sum((Y-Y_hat)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORWARD PROPAGATION (WELL THIS IS NOT A NN, BUT ...)\n",
    "def forward_prop(X, w, b):\n",
    "    \"\"\"\n",
    "    X is the input vector with m training examples, shape (1,m)\n",
    "    w: is the angular coefficient shape (9,1)\n",
    "    b: is the linear coefficient  shape (9,1)\n",
    "    Y_hat: a real number, the prediction \n",
    "    \"\"\"\n",
    "    Y_hat = np.sum(np.dot(w.T, X) + b)\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X,Y, Y_hat, m):\n",
    "    \"\"\"\n",
    "    Calculates the derivatives of the parameters\n",
    "    returns:\n",
    "    dw\n",
    "    db\n",
    "    \"\"\"\n",
    "    nx = X.shape[0]\n",
    "    ny = Y.shape[0]\n",
    "    dw = (-1./m)*np.dot(X,(Y -Y_hat).T)\n",
    "    db = (-1./m)*np.sum((Y -Y_hat))\n",
    "#     assert(dw.shape == (nx, ny))\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent Model\n",
    "Given the cost function $J(w,b)$, where w is a vector of weights that multiply each of the input features and b is a real constant number we want to minimize it.\n",
    "$$ J(w,b) = \\frac{1}{2m}\\sum_{i}^{m}(y^{(i)} - ŷ^{(i)})^{2} $$\n",
    "$$ ŷ = wx + b$$\n",
    "To use this algorithm we need to compute the derivatives of the cost (backward propagation) in order to update the parameters w and b.\n",
    "$$\\frac{\\partial J(w,b)}{\\partial w} = -\\frac{1}{m}\\sum_{i = i}^{m} (y - ŷ)x$$\n",
    "$$\\frac{\\partial J(w,b)}{\\partial w} = -\\frac{1}{m}\\sum_{i = i}^{m} (y - ŷ)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-11-cf4369f1a55e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-cf4369f1a55e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def GD_model(X,Y, num_iterations = 1000, learning_rate = 0.000000001, lambd):\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def GD_model(X,Y, num_iterations = 1000, learning_rate = 0.000000001, lambd):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input array shape (nx, m), nx is the number of input features and m the number of\n",
    "    training examples.\n",
    "    Y: label array shape (ny, m) where ny is the number of the output feature.\n",
    "    Given X and Y estimates the best linear function to fit the data\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    print('number of iterations: ', num_iterations)\n",
    "    print('m = ', m) \n",
    "    start_time = time.time()\n",
    "    w,b,dw,db = init_parameters(x_train)\n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        #forward propagation (we can think the activation function as linear)\n",
    "        Y_hat = forward_prop(X,w,b)\n",
    "        #plot the cost function\n",
    "        cost = l2_cost(X,Y, Y_hat,m)\n",
    "        plt.plot(iteration,cost,'ro')\n",
    "    \n",
    "        #Backward propagation calculate the derivatives \n",
    "        dw, db = backward_propagation(X,Y,Y_hat, m)\n",
    "        reg_term = (float(lambd)/m)*w\n",
    "        \n",
    "        #Update parameters simultaneusly\n",
    "        temp_w = w - learning_rate*dw\n",
    "        \n",
    "        #TO USE THE REGULARIZATION UNCOMMENT THE NEXT LINE\n",
    "        #temp_w = w - (learning_rate*dw + reg_term)\n",
    "        temp_b = b - learning_rate*db                    #AND COMMENT THIS\n",
    "        w = temp_w\n",
    "        b = temp_b\n",
    "    print('training time: %.2fs'  %(time.time() - start_time))\n",
    "    plt.show()\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIANIG THE GD\n",
    "num_iterations = 1000\n",
    "\n",
    "w_gd, b_gd, cost_gd= GD_model(x_train,y_train, num_iterations=num_iterations)\n",
    "print(\"cost in the training set: \", cost_gd)\n",
    "print(w_gd.shape)\n",
    "print(\"weights: \", w_gd)\n",
    "# print(\"bias: \", b_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_model(X,Y, num_iterations = 1000, learning_rate = 0.00000001, shuffle_model=True):\n",
    "    \"\"\"\n",
    "    X: an input array shape (nx, m), nx is the input feature size and m is the number of training examples\n",
    "    Given X and Y estimates the best linear function to fit the data\n",
    "    \"\"\"\n",
    "   \n",
    "    m = X.shape[1]    # number of training examples\n",
    "    nx = X.shape[0]   # number of features in the input\n",
    "    ny = Y.shape[0]   # number of features in the label\n",
    "    \n",
    "    # initialize parameters\n",
    "    w = np.random.randn(nx,1)*0.001\n",
    "    dw = np.random.randn(nx,1)*0.001\n",
    "    b = np.random.randn(1)*0.001\n",
    "    db = np.random.randn(1)*0.001\n",
    "    \n",
    "    # for each epoch\n",
    "    start_time = time.time()\n",
    "    for iteration in range(num_iterations):\n",
    "        if shuffle_model == True:\n",
    "            tmp1, tmp2 = shuffle(X.T, Y.T)\n",
    "            X, Y = tmp1.T, tmp2.T\n",
    "        print(\"\\riteração: \", iteration +1, end = \"\")\n",
    "       \n",
    "        Y_hat = np.empty((ny, m))\n",
    "        for i in range(m):\n",
    "            #extract a columns of the input, remember the ith column has the ith example\n",
    "            x_i = X[:, i].reshape(nx,1) #reshape to avoid rank 1 array\n",
    "            y_i = Y[:, i].reshape(ny,1)\n",
    "\n",
    "            #calulate the prediction y_hat\n",
    "            y_hat_i = np.dot(w.T,x_i) + b\n",
    "            Y_hat[:,i] = y_hat_i\n",
    "            \n",
    "            #calculate the derivatives\n",
    "            dw, db = backward_propagation(x_i,y_i,y_hat_i, 1)\n",
    "            reg_term = (float(lambd)/m)*w\n",
    "            \n",
    "            #TO USE REGULARIZATION UNCOMMENT THE NEXT LINE\n",
    "            #temp_w = w - (learning_rate*dw + reg_term)\n",
    "            temp_w = w - learning_rate*dw                  #AND COMMENT THIS ONE\n",
    "            temp_b = b - learning_rate*db\n",
    "            w = temp_w\n",
    "            b = temp_b\n",
    "        #calculate and plot the cost\n",
    "        cost = l2_cost(X,Y, Y_hat,m)\n",
    "        plt.ylabel('Custo')\n",
    "        plt.xlabel('Iteracoes')\n",
    "        plt.plot(iteration,cost,'ro')\n",
    "    print('\\ntraining time: %.2fs'  %(time.time() - start_time))\n",
    "    plt.show()\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING THE SDG MODEL\n",
    "# dividindo pelo máximo\n",
    "shuffle_model = True\n",
    "num_iterations = 1000\n",
    "\n",
    "w_sgd, b_sgd, cost_sgd = SGD_model(x_train, y_train, num_iterations=num_iterations, shuffle_model=shuffle_model)\n",
    "print(\"cost in the training set: \", cost_sgd)\n",
    "# print(\"weights: \", w_sgd)\n",
    "# print(\"bias: \", b_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "train_size = x_train.shape[1]\n",
    "\n",
    "clf = linear_model.SGDRegressor(loss='squared_loss', penalty='l2', learning_rate = 'constant', eta0 = 0.00000001)\n",
    "start_time = time.time()\n",
    "clf.fit(x_train.T, np.ravel(y_train.T))\n",
    "print('training time: %.2fs'  %(time.time() - start_time))\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_hat = clf.predict(x_train.T)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', clf.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"% mean_squared_error(y_train.T, y_hat.T))\n",
    "\n",
    "print(\"Error: %.2f\"\n",
    "      % l2_cost(x_train, y_train, y_hat, train_size))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_train.T, y_hat.T))\n",
    "\n",
    "# cost [element] = l2_cost(x_train, y_train, y_)\n",
    "# plt.show()\n",
    "# print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_norm = np.ones((train_size, 1))\n",
    "X_norm = np.concatenate((x0_norm, x_train.T), axis=1)\n",
    "# # compute Normal Eq.\n",
    "start_time = time.time()\n",
    "XTX_inv = np.linalg.inv(np.matmul(X_norm.T, X_norm))\n",
    "theta_norm = np.matmul(np.matmul(XTX_inv, X_norm.T), y_train.T)\n",
    "print('training time: %.2fs'  %(time.time() - start_time))\n",
    "print('Normal equation compute successfully =D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X_validation_norm = np.concatenate((np.ones((x_validation.shape[1], 1)), x_validation.T), axis=1)\n",
    "y_validation_norm = y_validation\n",
    "\n",
    "for i in range(X_validation_norm.shape[0]):\n",
    "    estimate = np.matmul(theta_norm.T, np.reshape(X_validation_norm[i], (X_norm.shape[1], 1)))\n",
    "    print('i: %d; estimado: %.3lf; real: %.3lf; diferenca: %.3lf' \n",
    "          %(i, estimate, y_validation_norm[:,i], estimate-y_validation_norm[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equations with regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGE THE LABDA VALUE HERE\n",
    "lambd = 0.01\n",
    "\n",
    "x0_norm_reg = np.ones((train_size, 1))\n",
    "nx = x_train.shape[0]\n",
    "print(\"nx = \", nx)\n",
    "i_matrix = np.identity(nx+1)\n",
    "i_matrix[0,0] = 0\n",
    "X_norm_reg = np.concatenate((x0_norm_reg, x_train.T), axis=1)\n",
    "# # compute Normal Eq.\n",
    "start_time = time.time()\n",
    "XTX_inv_reg = np.linalg.inv(np.matmul(X_norm_reg.T, X_norm_reg) + lambd*i_matrix)\n",
    "theta_norm_reg = np.matmul(np.matmul(XTX_inv_reg, X_norm_reg.T), y_train.T)\n",
    "print('training time: %.2fs'  %(time.time() - start_time))\n",
    "print('Normal equation compute successfully =D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MINIBATCH\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypotesis(theta, X):\n",
    "    return np.matmul(theta.T, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epoch = 100\n",
    "batches_per_epoch = int(np.ceil(train_size / float(batch_size)))\n",
    "n_steps = 1000\n",
    "learning_rate = 0.000000001\n",
    "cost_mbgd = np.zeros((n_steps))\n",
    "\n",
    "print('training size: ', train_size)\n",
    "print('number of epoch: ', n_epoch)\n",
    "print('batches per epoch: ', batches_per_epoch)\n",
    "print('number of steps: ', n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE LAMBD VALUE HERE\n",
    "lambd = 0.01\n",
    "theta_mbgd = np.random.rand(x_train.shape[0] + 1, 1) * 0.01\n",
    "x0_mbgd = np.ones((train_size, 1))\n",
    "X_mbgd = np.concatenate((x0_mbgd, x_train.T), axis=1)\n",
    "y_train_mbgd = np.copy(y_train)\n",
    "\n",
    "for j in range(n_steps):\n",
    "    X_mbgd, tmp = shuffle(X_mbgd, y_train_mbgd.T)\n",
    "    y_train_mbgd = tmp.T\n",
    "    for i in range(0, train_size, batch_size):\n",
    "        h = hypotesis(theta_mbgd, X_mbgd[i:i+batch_size, :].T)\n",
    "        cost_mbgd[j] = l2_cost(theta_mbgd, h, y_train_mbgd[:, i:i+batch_size], batch_size)\n",
    "        reg_term = (float(lambd)/batch_size)*w\n",
    "        theta_mbgd = theta_mbgd - (learning_rate  * np.sum(np.dot((h - y_train_mbgd[:, i:i+batch_size]), X_mbgd[i:i+batch_size, :]))) / batch_size\n",
    "        #TO USE REGULARIZATION UNCOMMENT THE NEXT LINE\n",
    "#         theta_mbgd = theta_mgdb - learning_rate*reg_term\n",
    "    if j % 50:\n",
    "        print('j: %d;\\tcost: %.3lf' %(j, cost_mbgd[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_mbgd = np.concatenate((np.ones((x_validation.shape[1], 1)), x_validation.T), axis=1)\n",
    "y_validation_mbgd = y_validation\n",
    "\n",
    "print('*' * 40)\n",
    "\n",
    "for i in range(len(X_validation_mbgd)):\n",
    "    estimate = np.matmul(theta_mbgd.T, np.reshape(X_validation_mbgd[i], (X_mbgd.shape[1], 1)))\n",
    "    print('i: %d; estimado: %.3lf; real: %.3lf; diferenca: %.3lf' \n",
    "          %(i, estimate, y_validation_mbgd[:,i], estimate-y_validation_mbgd[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Custo')\n",
    "plt.xlabel('Iteracoes')\n",
    "plt.plot(range(cost_mbgd.shape[0]), cost_mbgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "Now, we had already trained our model on training set. We will use the parameters obtained to estimate the model's error on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = y_validation.shape[1]\n",
    "print(\"validation size = \", validation_size)\n",
    "gd_y_hat = np.dot(w_gd.T, x_validation) + b_gd\n",
    "gd_cost = l2_cost(x_test, gd_y_hat, y_validation, validation_size)\n",
    "print(\"The cost on validation set GD: \", gd_cost)\n",
    "\n",
    "sgd_y_hat = np.dot(w_sgd.T, x_validation) + b_sgd\n",
    "sgd_cost = l2_cost(x_test, sgd_y_hat, y_validation, validation_size)\n",
    "print(\"The cost on validation set SGD: \", sgd_cost)\n",
    "\n",
    "norm_y_hat = np.dot(theta_norm.T, np.concatenate((np.ones((1, validation_size)),x_validation), axis=0))\n",
    "norm_cost = l2_cost(x_test, norm_y_hat, y_validation, validation_size)\n",
    "print(\"The cost on validation set Normal Equation: \", norm_cost)\n",
    "\n",
    "mbgd_y_hat = np.dot(theta_mbgd.T, np.concatenate((np.ones((1, validation_size)),x_validation), axis=0))\n",
    "mbgd_cost = l2_cost(x_test, mbgd_y_hat, y_validation, validation_size)\n",
    "print(\"The cost on validation set mbgd: \", mbgd_cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(validation_size)\n",
    "x_index = x_validation[:, index]\n",
    "y_index = y_validation[:, index]\n",
    "gd_y_predict = np.dot(w_gd.T, x_index) + b_gd\n",
    "sgd_y_predict = np.dot(w_sgd.T, x_index) + b_sgd\n",
    "print('index = ', index)\n",
    "print('the input: ', x_index)\n",
    "print('gd_prediction: ', gd_y_predict)\n",
    "print('sgd_prediction: ', sgd_y_predict)\n",
    "print('true label: ', y_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
