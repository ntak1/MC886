{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC886 Assigment 1\n",
    "\n",
    "## Objective:\n",
    "Use linear regression model to predict the prices of diamonds given their atributes. The data set might be found on:\n",
    "https://www.kaggle.com/shivam2503/diamonds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/diamonds.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir, 'rb') as csvfile:\n",
    "    dataset = pd.read_csv(data_dir)\n",
    "    \n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carat\tcut\tcolor\tclarity\tdepth\ttable\tprice\tx\ty\tz\n",
    "# print(dataset['cut'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "### 1. Split data in training, validation and test\n",
    "A wise person said: \"friends donâ€™t let friends use testing data for training\". t.\n",
    "\n",
    "**Note:**\n",
    "* **sklearn.model_selection.train_test_split(*arrays, **options)**  \n",
    "    Split arrays or matrices into random train and test subsets.  \n",
    "    See documentation:http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "### 2. Put the dataset into Numpy volume\n",
    "Categorical atributes are changed to integers.  \n",
    "The cell bellow runs a code to shuffle and split the dataset into 3 sets, training, validation and test.\n",
    "\n",
    "### 3. Transform categorical data into numbers\n",
    "Used fit_transform function.\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html  \n",
    "http://pbpython.com/categorical-encoding.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS ONLY ONCE!\n",
    "# this code saves the numpy vectors to files\n",
    "# split_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53940, 11)\n",
      "0        0.23\n",
      "1        0.21\n",
      "2        0.23\n",
      "3        0.29\n",
      "4        0.31\n",
      "5        0.24\n",
      "6        0.24\n",
      "7        0.26\n",
      "8        0.22\n",
      "9        0.23\n",
      "10       0.30\n",
      "11       0.23\n",
      "12       0.22\n",
      "13       0.31\n",
      "14       0.20\n",
      "15       0.32\n",
      "16       0.30\n",
      "17       0.30\n",
      "18       0.30\n",
      "19       0.30\n",
      "20       0.30\n",
      "21       0.23\n",
      "22       0.23\n",
      "23       0.31\n",
      "24       0.31\n",
      "25       0.23\n",
      "26       0.24\n",
      "27       0.30\n",
      "28       0.23\n",
      "29       0.23\n",
      "         ... \n",
      "53910    0.70\n",
      "53911    0.57\n",
      "53912    0.61\n",
      "53913    0.80\n",
      "53914    0.84\n",
      "53915    0.77\n",
      "53916    0.74\n",
      "53917    0.90\n",
      "53918    0.76\n",
      "53919    0.76\n",
      "53920    0.70\n",
      "53921    0.70\n",
      "53922    0.70\n",
      "53923    0.73\n",
      "53924    0.73\n",
      "53925    0.79\n",
      "53926    0.71\n",
      "53927    0.79\n",
      "53928    0.79\n",
      "53929    0.71\n",
      "53930    0.71\n",
      "53931    0.71\n",
      "53932    0.70\n",
      "53933    0.70\n",
      "53934    0.72\n",
      "53935    0.72\n",
      "53936    0.72\n",
      "53937    0.70\n",
      "53938    0.86\n",
      "53939    0.75\n",
      "Name: carat, Length: 53940, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# lb_make = LabelEncoder()\n",
    "# print(dataset.shape)\n",
    "# column = np.empty((dataset.shape[0],1))\n",
    "# column= lb_make.fit_transform(dataset['cut'])\n",
    "# # print(dataset['cut'])\n",
    "# print(dataset['carat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the numpy files (training set, validation set, test set)\n",
    "x_train, y_train, x_validation, y_validation, x_test, y_test = load_vectors()\n",
    "y_train = y_train.reshape((1,y_train.shape[0]))\n",
    "y_validation = y_validation.reshape((1,y_validation.shape[0]))\n",
    "y_test = y_test.reshape((1,y_test.shape[0]))\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_validation.shape)\n",
    "print(y_validation.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(X):\n",
    "    \"\"\"\n",
    "    Receives: the input vector shape (nx, m), where nx is the number of features and\n",
    "    m the number of examples.\n",
    "    Returns:\n",
    "    W: the weight array shape ( 1, nx)\n",
    "    dW: the derivative array shape ( 1, nx)\n",
    "    b: a real number (bias)\n",
    "    db: the derivative of the bias.\n",
    "    \"\"\"\n",
    "    nx = X.shape[0]\n",
    "    W = np.random.randn(nx,1)*0.01\n",
    "    dw = np.random.randn(nx,1)*0.01\n",
    "    b = 0.\n",
    "    db = 0.\n",
    "    return W,b, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_cost(X, Y, Y_hat, m):\n",
    "    \"\"\"\n",
    "    Cost function defined by a variation of the l2 norm function.\n",
    "    The '2' in the denominator makes its deriative easier\n",
    "    \"\"\"\n",
    "    cost = (1/(2*m))*np.sum((Y-Y_hat)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FORWARD PROPAGATION (WELL THIS IS NOT A NN, BUT ...)\n",
    "def forward_prop(X, w, b):\n",
    "    \"\"\"\n",
    "    X is the input vector with m training examples, shape (1,m)\n",
    "    w: is the angular coefficient shape (9,1)\n",
    "    b: is the linear coefficient  shape (9,1)\n",
    "    \"\"\"\n",
    "    Y_hat = np.dot(w.T, X) + b\n",
    "    return Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X,Y, Y_hat, m):\n",
    "    \"\"\"\n",
    "    Calculates the derivatives of the parameters\n",
    "    returns:\n",
    "    dw\n",
    "    db\n",
    "    \"\"\"\n",
    "    nx = X.shape[0]\n",
    "    ny = Y.shape[0]\n",
    "    dw = (-1./m)*np.dot(X,(Y -Y_hat).T)\n",
    "    db = (-1./m)*np.sum((Y -Y_hat))\n",
    "    assert(dw.shape == (nx, ny))\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_model(X,Y, num_iterations = 100, learning_rate = 0.000000001):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X: input array shape (nx, m), nx is the number of input features and m the number of\n",
    "    training examples.\n",
    "    Y: label array shape (ny, m) where ny is the number of the output feature.\n",
    "    Given X and Y estimates the best linear function to fit the data\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    print('m = ', m) \n",
    "    \n",
    "    w,b,dw,db = init_parameters(x_train)\n",
    "    for iteration in range(num_iterations):\n",
    "        \n",
    "        #forward propagation (we can think the activation function as linear)\n",
    "        Y_hat = forward_prop(X,w,b)\n",
    "        \n",
    "        #plot the cost function\n",
    "        cost = l2_cost(X,Y, Y_hat,m)\n",
    "        plt.plot(iteration,cost,'ro')\n",
    "    \n",
    "        #Backward propagation calculate the derivatives \n",
    "        dw, db = backward_propagation(X,Y,Y_hat, m)\n",
    "    \n",
    "        #Update parameters simultaneusly\n",
    "        temp_w = w - learning_rate*dw\n",
    "        temp_b = b - learning_rate*db\n",
    "        w = temp_w\n",
    "        b = temp_b\n",
    "    plt.show()\n",
    "    print(\"dw.shape = \", dw.shape)\n",
    "    print(\"db.shape = \", db.shape)\n",
    "    print(\"w.shape = \", w.shape)\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_gd, b_gd, cost_gd= GD_model(x_train,y_train)\n",
    "print(\"cost in the training set: \", cost_gd)\n",
    "print(\"weights: \", w_gd)\n",
    "print(\"bias: \", b_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_model(X,Y, num_iterations = 20, learning_rate = 0.00000001):\n",
    "    \"\"\"\n",
    "    X: an input array shape (nx, m), nx is the input feature size and m is the number of training examples\n",
    "    Given X and Y estimates the best linear function to fit the data\n",
    "    \"\"\"\n",
    "    # initialize parameters\n",
    "    w,b,dw,db = init_parameters(x_train)\n",
    "    print(\"first \", dw)\n",
    "    m = X.shape[1]    # number of training examples\n",
    "    nx = X.shape[0]   # number of features in the input\n",
    "    ny = Y.shape[0]   # number of features in the label\n",
    "    \n",
    "    # for each epoch\n",
    "    for iteration in range(num_iterations):\n",
    "        print(\"\\riteraÃ§Ã£o: \", iteration +1, end = \"\")\n",
    "        #for each element in the example's vector (shape (9,1))\n",
    "        Y_hat = np.empty((ny, m))\n",
    "        for i in range(m):\n",
    "            #extract a columns of the input, remember the ith column has the ith example\n",
    "            x_i = X[:, i].reshape(nx,1) #reshape to avoid rank 1 array\n",
    "            y_i = Y[:, i].reshape(ny,1)\n",
    "#             print(\"x_i.shape = \", x_i.shape)\n",
    "#             print(\"y_i.shape = \", y_i.shape)\n",
    "#             print(\"w.shape = \", w.shape)\n",
    "            \n",
    "            #calulate the prediction y_hat\n",
    "            y_hat_i = np.dot(w.T,x_i) + b\n",
    "            Y_hat[:,i] = y_hat_i\n",
    "#             print(\"y_hat_i.shape = \", y_hat_i.shape)\n",
    "            \n",
    "#             #calculate the derivatives\n",
    "#              dw = (-1./m)*np.dot(X,(Y -Y_hat).T)\n",
    "#     db = (-1./m)*np.sum((Y -Y_hat))\n",
    "            \n",
    "#             dw = \n",
    "            dw, db = backward_propagation(x_i,y_i,y_hat_i, 1)\n",
    "#             print(\"dw.shape = \", dw.shape)\n",
    "#             print(\"db.shape = \", db.shape)\n",
    "            #Update parameters simultaneusly\n",
    "            temp_w = w - learning_rate*dw\n",
    "            temp_b = b - learning_rate*db\n",
    "            w = temp_w\n",
    "            b = temp_b\n",
    "            #plot the cost function\n",
    "#             cost = (1./(2.))*np.sum((label -Y_hat)**2)\n",
    "        cost = l2_cost(X,Y, Y_hat,m)\n",
    "        print(\"cost = \", cost)\n",
    "        plt.plot(iteration,cost,'ro')\n",
    "    plt.show()\n",
    "    return w, b, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_sgd, b_sgd, cost_sgd = SGD_model(x_train,y_train)\n",
    "print(\"cost in the training set: \", cost_sgd)\n",
    "print(\"weights: \", w_sgd)\n",
    "print(\"bias: \", b_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "clf = linear_model.SGDRegressor(loss='squared_loss', penalty='l2', learning_rate = 'constant', eta0 = 0.00000001)\n",
    "clf.fit(x_train.T, y_train.T)\n",
    "# Make predictions using the testing set\n",
    "y_hat = clf.predict(x_train.T)\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', clf.coef_)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train.T, y_hat.T))\n",
    "train_size = 30000\n",
    "print(\"Error: %.2f\"\n",
    "      % l2_cost(x_train, y_train, y_hat, train_size))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_train.T, y_hat.T))\n",
    "\n",
    "# cost [element] = l2_cost(x_train, y_train, y_)\n",
    "# plt.show()\n",
    "# print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_norm = np.concatenate((x_train, x_validation), axis=1).T\n",
    "\n",
    "x0_norm = np.ones((1, train_size))\n",
    "X_norm = np.concatenate((x0_norm, x_train), axis=0)\n",
    "print(X_norm.shape)\n",
    "# y_norm = np.concatenate((y_train, y_validation), axis=1).T\n",
    "# n_features = X_norm.shape[1]\n",
    "# print('X:', X_norm.shape)\n",
    "# # compute Normal Eq.\n",
    "XTX_inv = np.linalg.inv(np.matmul(X_norm.T, X_norm))\n",
    "theta_norm = np.matmul(np.matmul(XTX_inv, X_norm.T), y_train)\n",
    "# print('theta: ', theta_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X_test_norm = np.concatenate((np.ones((x_test.shape[1], 1)), x_test.T), axis=1)\n",
    "y_test_norm = y_test\n",
    "\n",
    "print('test')\n",
    "print('X test:', X_test_norm.shape)\n",
    "print('y test:', y_test_norm.shape)\n",
    "print('*' * 40)\n",
    "\n",
    "max_diff = 0\n",
    "for i in range(len(X_test_norm)):\n",
    "    estimate = np.matmul(theta_norm.T, np.reshape(X_test_norm[i], (n_features, 1)))\n",
    "    print('i: %d; estimado: %lf; real: %lf; diferenca: %lf' \n",
    "          %(i, estimate, y_test_norm[:,i], estimate-y_test_norm[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypotesis(theta, X):\n",
    "    return np.matmul(theta.T, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epoch = 100\n",
    "batches_per_epoch = int(np.ceil(train_size / float(batch_size)))\n",
    "n_steps = int(round(batches_per_epoch * n_epoch))\n",
    "learning_rate = 0.000000001\n",
    "\n",
    "print('training size: ', train_size)\n",
    "print('number of epoch: ', n_epoch)\n",
    "print('batches per epoch: ', batches_per_epoch)\n",
    "print('number of steps: ', n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "theta_mbgd = np.random.rand(nx + 1, ny)\n",
    "x0_mbgd = np.ones((train_size, 1))\n",
    "X_mbgd = np.concatenate((x0_mbgd, x_train.T), axis=1)\n",
    "print(theta_mbgd.shape,  X_mbgd.shape)\n",
    "\n",
    "for j in range(10):\n",
    "#     np.random.shuffle(X_mbgd)\n",
    "    for i in range(n_epoch):\n",
    "        h = hypotesis(theta_mbgd, X_mbgd[i:i+batch_size, :].T)\n",
    "    #    print(h.shape)\n",
    "        cost = np.sum((h - np.sum(y_train[:, i:i+batch_size]))) / batch_size\n",
    "\n",
    "        # update\n",
    "        theta_mbgd = theta_mbgd - (learning_rate  * cost)\n",
    "#         if j%10000==0 and i == 0:\n",
    "        print('j: %d;\\tcost: %.3lf' %(j, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "X_test_mbgd = np.concatenate((np.ones((x_test.shape[1], 1)), x_test.T), axis=1)\n",
    "y_test_mbgd = y_test\n",
    "\n",
    "print('test')\n",
    "print('X test:', X_test_mbgd.shape)\n",
    "print('y test:', y_test_mbgd.shape)\n",
    "print('*' * 40)\n",
    "\n",
    "max_diff = 0\n",
    "for i in range(len(X_test_mbgd)):\n",
    "    estimate = np.matmul(theta_mbgd.T, np.reshape(X_test_mbgd[i], (nx+1, 1)))\n",
    "    print('i: %d; estimado: %.3lf; real: %.3lf; diferenca: %.3lf' \n",
    "          %(i, estimate, y_test_mbgd[:,i], estimate-y_test_mbgd[:,i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
